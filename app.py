from flask import Flask, render_template, request, jsonify
import os
import re

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs('model', exist_ok=True)

# ==================== ML –ò–ú–ü–û–†–¢–´ ====================

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import LogisticRegression
    import joblib
    import numpy as np
    ML_AVAILABLE = True
    print("‚úÖ ML –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
except ImportError as e:
    ML_AVAILABLE = False
    print(f"‚ö†Ô∏è ML –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")

# ==================== –ì–ò–ë–†–ò–î–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† ====================

def hybrid_sentiment_analyzer(text):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: rule-based + ML"""
    text_lower = text.lower().strip()
    
    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    if not text_lower or len(text_lower) < 3:
        return "–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–û", 0.5, "üòê"
    
    # –°–ò–õ–¨–ù–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ (–ø—Ä–∞–≤–∏–ª–∞ - –í–´–°–û–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢)
    strong_negative = [
        '—Ö—É–π–Ω—è', '–≥–æ–≤–Ω–æ', '–ø–∏–∑–¥–µ—Ü', '–¥–µ—Ä—å–º–æ', '–æ—Ç—Å—Ç–æ–π', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π',
        '—É–∂–∞—Å–Ω—ã–π', '–∫–æ—à–º–∞—Ä', '–ø—Ä–æ–≤–∞–ª', '–º—É–¥–∞–∫', '–≥–∞–Ω–¥–æ–Ω', '—É–±–ª—é–¥–æ–∫', '–ø–∏–¥–æ—Ä–∞—Å', '–∑–∞–ª—É–ø–∞'
    ]
    
    strong_positive = [
        '—à–µ–¥–µ–≤—Ä', '–±–ª–µ—Å—Ç—è—â–∏–π', '–≥–µ–Ω–∏–∞–ª—å–Ω—ã–π', '–≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–π', '–∏–¥–µ–∞–ª—å–Ω—ã–π', '–±–µ–∑—É–ø—Ä–µ—á–Ω—ã–π',
        '—Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã–π', '–Ω–µ–∑–∞–±—ã–≤–∞–µ–º—ã–π', '–≤—ã–¥–∞—é—â–∏–π—Å—è', '–ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π', '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–π'
    ]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –°–ò–õ–¨–ù–´–• –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    strong_neg_count = sum(1 for word in strong_negative if word in text_lower)
    if strong_neg_count >= 1:
        confidence = min(0.85 + (strong_neg_count * 0.03), 0.98)
        return "–ù–ï–ì–ê–¢–ò–í–ù–´–ô", confidence, "üò†"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –°–ò–õ–¨–ù–´–• –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    strong_pos_count = sum(1 for word in strong_positive if word in text_lower)
    if strong_pos_count >= 1:
        confidence = min(0.85 + (strong_pos_count * 0.03), 0.98)
        return "–ü–û–ó–ò–¢–ò–í–ù–´–ô", confidence, "üòä"
    
    # –ï—Å–ª–∏ ML –¥–æ—Å—Ç—É–ø–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
    if ML_AVAILABLE and len(text_lower.split()) >= 3:
        try:
            ml_result = ml_analyze_sentiment(text)
            # –ï—Å–ª–∏ ML —É–≤–µ—Ä–µ–Ω - –¥–æ–≤–µ—Ä—è–µ–º –µ–º—É
            if ml_result[1] > 0.7:
                return ml_result
        except Exception as e:
            print(f"‚ö†Ô∏è ML –∞–Ω–∞–ª–∏–∑ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
    
    # Fallback –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
    return advanced_rule_based_analyzer(text)

def advanced_rule_based_analyzer(text):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π rule-based –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    text_lower = text.lower()
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ —Å –≤–µ—Å–∞–º–∏
    positive_words = {
        '–æ—Ç–ª–∏—á–Ω—ã–π': 2, '–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π': 2, '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π': 2, '—Å—É–ø–µ—Ä': 1, '–∫–ª–∞—Å—Å–Ω—ã–π': 1,
        '—à–∏–∫–∞—Ä–Ω—ã–π': 2, '–≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–π': 2, '–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ': 2, '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π': 1,
        '–ª—é–±–ª—é': 2, '–æ–±–æ–∂–∞—é': 2, '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é': 2, '—Å–æ–≤–µ—Ç—É—é': 2, '–Ω—Ä–∞–≤–∏—Ç—Å—è': 1,
        '–≤–æ—Å—Ç–æ—Ä–≥': 2, '—É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ': 1, '—Ç–∞–ª–∞–Ω—Ç–ª–∏–≤—ã–π': 1, '–º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ': 2,
        '–∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π': 2, '—Ç—Ä–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π': 1, '–≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π': 2, '–≥–ª—É–±–æ–∫–∏–π': 1,
        '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π': 1, '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π': 1, '–¥–∏–Ω–∞–º–∏—á–Ω—ã–π': 1, '–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π': 1
    }
    
    negative_words = {
        '—É–∂–∞—Å–Ω—ã–π': 2, '–ø–ª–æ—Ö–æ–π': 1, '—Å–∫—É—á–Ω—ã–π': 2, '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π': 2, '–∫–æ—à–º–∞—Ä': 2,
        '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ': 2, '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–ª': 2, '–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é': 3, '–Ω–µ —Å–æ–≤–µ—Ç—É—é': 3,
        '—Å–∫—É—á–Ω–æ–≤–∞—Ç–æ': 1, '–∑–∞—Ç—è–Ω—É—Ç–æ': 1, '–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ': 1, '—Å–ª–∞–±—ã–π': 1, '—Å–ª–∞–±–∞—è': 1,
        '–Ω–µ —Å—Ç–æ–∏—Ç': 2, '–∂–∞–ª–∫–æ –≤—Ä–µ–º–µ–Ω–∏': 3, '–∂–∞–ª–∫–æ –¥–µ–Ω–µ–≥': 2, '–æ–∂–∏–¥–∞–ª –±–æ–ª—å—à–µ–≥–æ': 2,
        '–Ω–µ—É–¥–∞—á–Ω—ã–π': 1, '–ø—Ä–æ–≤–∞–ª': 2, '—Å–∫—É—á–Ω–æ': 2, '–ø–ª–æ—Ö–æ': 1, '–Ω–µ–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ': 1
    }
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ—Ä–∞–∑—ã (–≤—ã—Å–æ–∫–∏–π –≤–µ—Å)
    positive_phrases = [
        '–Ω–∞ –æ–¥–Ω–æ–º –¥—ã—Ö–∞–Ω–∏–∏', '–∞–∫—Ç–µ—Ä—Å–∫–∞—è –∏–≥—Ä–∞ –Ω–∞ –≤—ã—Å–æ—Ç–µ', '—Å–º–æ—Ç—Ä–µ–ª –Ω–∞ –æ–¥–Ω–æ–º –¥—ã—Ö–∞–Ω–∏–∏',
        '–ª—É—á—à–∏–π —Ñ–∏–ª—å–º –≥–æ–¥–∞', '–ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑', '–æ—Å—Ç–∞–ª—Å—è –ø–æ–¥ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ–º',
        '—Ü–µ–ø–ª—è–µ—Ç —Å –ø–µ—Ä–≤—ã—Ö –º–∏–Ω—É—Ç', '–Ω–µ –æ—Ç–ø—É—Å–∫–∞–µ—Ç –¥–æ –∫–æ–Ω—Ü–∞', '–æ–ø–µ—Ä–∞—Ç–æ—Ä—Å–∫–∞—è —Ä–∞–±–æ—Ç–∞ –≤—ã—à–µ –≤—Å—è–∫–∏—Ö –ø–æ—Ö–≤–∞–ª'
    ]
    
    negative_phrases = [
        '–∑—Ä—è –ø–æ—Ç—Ä–∞—Ç–∏–ª –≤—Ä–µ–º—è', '—Å—é–∂–µ—Ç–Ω—ã–µ –¥—ã—Ä—ã', '–º–æ–∂–Ω–æ –±—ã–ª–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å',
        '–ø–µ—Ä—Å–æ–Ω–∞–∂–∏ –∫–∞—Ä—Ç–æ–Ω–Ω—ã–µ', '–¥–∏–∞–ª–æ–≥–∏ –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ', '—Å–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç—ã –≤—ã–≥–ª—è–¥—è—Ç –¥–µ—à–µ–≤–æ',
        '–∫–æ–Ω—Ü–æ–≤–∫–∞ –∏—Å–ø–æ—Ä—Ç–∏–ª–∞ –≤–µ—Å—å —Ñ–∏–ª—å–º', '–æ–∂–∏–¥–∞–ª –±–æ–ª—å—à–µ–≥–æ –Ω–æ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–ª—Å—è'
    ]
    
    # –ü–æ–¥—Å—á–µ—Ç –æ—á–∫–æ–≤
    positive_score = 0
    negative_score = 0
    
    # –°–ª–æ–≤–∞
    for word, weight in positive_words.items():
        if word in text_lower:
            positive_score += weight
    
    for word, weight in negative_words.items():
        if word in text_lower:
            negative_score += weight
    
    # –§—Ä–∞–∑—ã (–≤—ã—Å–æ–∫–∏–π –≤–µ—Å)
    for phrase in positive_phrases:
        if phrase in text_lower:
            positive_score += 3
    
    for phrase in negative_phrases:
        if phrase in text_lower:
            negative_score += 3
    
    # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    neutral_words = ['—Å—Ä–µ–¥–Ω–µ', '–Ω–æ—Ä–º–∞–ª—å–Ω–æ', '–æ–±—ã—á–Ω–æ', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ', '–Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ', '—Ç–∞–∫ —Å–µ–±–µ']
    neutral_count = sum(1 for word in neutral_words if word in text_lower)
    
    # –õ–æ–≥–∏–∫–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
    total_score = positive_score - negative_score
    
    if neutral_count >= 2 and abs(total_score) < 3:
        return "–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–û", 0.5, "üòê"
    
    if total_score > 5:
        confidence = min(0.75 + (total_score * 0.04), 0.95)
        return "–ü–û–ó–ò–¢–ò–í–ù–´–ô", confidence, "üòä"
    elif total_score < -5:
        confidence = min(0.75 + (abs(total_score) * 0.04), 0.95)
        return "–ù–ï–ì–ê–¢–ò–í–ù–´–ô", confidence, "üò†"
    elif total_score > 2:
        confidence = 0.65 + (total_score * 0.05)
        return "–ü–û–ó–ò–¢–ò–í–ù–´–ô", confidence, "üôÇ"
    elif total_score < -2:
        confidence = 0.65 + (abs(total_score) * 0.05)
        return "–ù–ï–ì–ê–¢–ò–í–ù–´–ô", confidence, "üòê"
    else:
        return "–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–û", 0.5, "üòê"

def ml_analyze_sentiment(text):
    """ML –∞–Ω–∞–ª–∏–∑ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–±—É—á–µ–Ω–∏–µ–º"""
    try:
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
        try:
            model = joblib.load('model/sentiment_model.pkl')
            vectorizer = joblib.load('model/vectorizer.pkl')
            print("‚úÖ ML –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")
        except:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ—Ç - —Å–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é
            print("üîÑ –°–æ–∑–¥–∞–µ–º ML –º–æ–¥–µ–ª—å...")
            model, vectorizer = create_enhanced_ml_model()
        
        # –ê–Ω–∞–ª–∏–∑
        text_vector = vectorizer.transform([text])
        prediction = model.predict(text_vector)[0]
        probability = model.predict_proba(text_vector)[0]
        
        confidence = probability[prediction]
        
        if prediction == 1:
            return "–ü–û–ó–ò–¢–ò–í–ù–´–ô", confidence, "üòä"
        else:
            return "–ù–ï–ì–ê–¢–ò–í–ù–´–ô", confidence, "üò†"
            
    except Exception as e:
        print(f"‚ùå ML –æ—à–∏–±–∫–∞: {e}")
        raise

def create_enhanced_ml_model():
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é ML –º–æ–¥–µ–ª—å –Ω–∞ –ª–µ—Ç—É"""
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    positive_texts = [
        "–§–∏–ª—å–º –ø—Ä–æ—Å—Ç–æ –≤–µ–ª–∏–∫–æ–ª–µ–ø–µ–Ω! –ê–∫—Ç–µ—Ä—ã –∏–≥—Ä–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ.",
        "–û—Ç–ª–∏—á–Ω—ã–π —Ñ–∏–ª—å–º! –°–º–æ—Ç—Ä–µ–ª –Ω–∞ –æ–¥–Ω–æ–º –¥—ã—Ö–∞–Ω–∏–∏.",
        "–®–µ–¥–µ–≤—Ä! –õ—É—á—à–µ–µ —á—Ç–æ —è –≤–∏–¥–µ–ª –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è.",
        "–ü—Ä–µ–∫—Ä–∞—Å–Ω–∞—è –æ–ø–µ—Ä–∞—Ç–æ—Ä—Å–∫–∞—è —Ä–∞–±–æ—Ç–∞ –∏ –≥–ª—É–±–æ–∫–∞—è –¥—Ä–∞–º–∞.",
        "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–µ—Ä—Å–∫–∞—è –∏–≥—Ä–∞, –ø—Ä–æ—Å—Ç–æ –≤–∞—É!",
        "–°—é–∂–µ—Ç –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π, –Ω–µ –æ—Ç–æ—Ä–≤–∞—Ç—å—Å—è.",
        "–ì–ª—É–±–æ–∫–∏–π –∏ —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–π —Ñ–∏–ª—å–º.",
        "–ù–∞–ø—Ä—è–∂–µ–Ω–Ω—ã–π —Ç—Ä–∏–ª–ª–µ—Ä —Å –æ—Ç–ª–∏—á–Ω–æ–π –∞—Ç–º–æ—Å—Ñ–µ—Ä–æ–π.",
        "–¢—Ä–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –æ –ª—é–±–≤–∏ –∏ –ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç–∏.",
        "–ò–¥–µ–∞–ª—å–Ω—ã–π —Ñ–∏–ª—å–º –¥–ª—è –≤–µ—á–µ—Ä–Ω–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞.",
        "–û—Ç–ª–∏—á–Ω—ã–π –∫–∞—Å—Ç! –í—Å–µ –∞–∫—Ç–µ—Ä—ã –ø–æ–¥–æ–±—Ä–∞–Ω—ã –∏–¥–µ–∞–ª—å–Ω–æ.",
        "–§–∏–ª—å–º —Ü–µ–ø–ª—è–µ—Ç —Å –ø–µ—Ä–≤—ã—Ö –º–∏–Ω—É—Ç.",
        "–û—Å—Ç–∞–ª—Å—è –ø–æ–¥ –±–æ–ª—å—à–∏–º –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ–º.",
        "–†–µ–∫–æ–º–µ–Ω–¥—É—é –≤—Å–µ–º –∫ –ø—Ä–æ—Å–º–æ—Ç—Ä—É!",
        "–ü–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞–ª —É–∂–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑.",
        "–°—é–∂–µ—Ç —Å –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–º–∏ –ø–æ–≤–æ—Ä–æ—Ç–∞–º–∏.",
        "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –ø–æ—Ä–∞–∂–∞–µ—Ç.",
        "–ì–µ—Ä–æ–∏ –≤—ã–∑—ã–≤–∞—é—Ç —Å–∏–º–ø–∞—Ç–∏—é —Å –ø–µ—Ä–≤—ã—Ö –º–∏–Ω—É—Ç.",
        "–î–∏–Ω–∞–º–∏—á–Ω—ã–π —Å—é–∂–µ—Ç, –Ω–µ—Ç —Å–∫—É—á–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤.",
        "–ë–µ—Ä–µ—Ç –∑–∞ –¥—É—à—É, –Ω–µ –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–∞–≤–Ω–æ–¥—É—à–Ω—ã–º."
    ]
    
    negative_texts = [
        "–£–∂–∞—Å–Ω—ã–π —Ñ–∏–ª—å–º! –ü–æ–ª–Ω–æ–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ.",
        "–°–∫—É—á–Ω–æ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ, –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é.",
        "–ó—Ä—è –ø–æ—Ç—Ä–∞—Ç–∏–ª –≤—Ä–µ–º—è –Ω–∞ —ç—Ç–æ—Ç —Ñ–∏–ª—å–º.",
        "–ü–ª–æ—Ö–∞—è –∞–∫—Ç–µ—Ä—Å–∫–∞—è –∏–≥—Ä–∞ –∏ —Å–ª–∞–±—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π.",
        "–°—é–∂–µ—Ç–Ω—ã–µ –¥—ã—Ä—ã –≤–∏–¥–Ω—ã –Ω–µ–≤–æ–æ—Ä—É–∂–µ–Ω–Ω—ã–º –≥–ª–∞–∑–æ–º.",
        "–ó–∞—Ç—è–Ω—É—Ç–æ –∏ —Å–∫—É—á–Ω–æ–≤–∞—Ç–æ.",
        "–û–∂–∏–¥–∞–ª –±–æ–ª—å—à–µ–≥–æ, –Ω–æ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–ª—Å—è.",
        "–î–∏–∞–ª–æ–≥–∏ –∑–≤—É—á–∞—Ç –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.",
        "–°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç—ã –≤—ã–≥–ª—è–¥—è—Ç –¥–µ—à–µ–≤–æ.",
        "–ö–æ–Ω—Ü–æ–≤–∫–∞ –∏—Å–ø–æ—Ä—Ç–∏–ª–∞ –≤–µ—Å—å —Ñ–∏–ª—å–º.",
        "–ü–µ—Ä—Å–æ–Ω–∞–∂–∏ –∫–∞—Ä—Ç–æ–Ω–Ω—ã–µ, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–ø–µ—Ä–µ–∂–∏–≤–∞—Ç—å.",
        "–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–ª–∏—à–µ –∏ —à—Ç–∞–º–ø–æ–≤.",
        "–¢–µ–º–ø –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π, —Ç–æ –±—ã—Å—Ç—Ä–æ —Ç–æ –º–µ–¥–ª–µ–Ω–Ω–æ.",
        "–ú—É–∑—ã–∫–∞ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –∫ —Å—Ü–µ–Ω–∞–º.",
        "–ê–∫—Ç–µ—Ä—ã —è–≤–Ω–æ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è —Ä–æ–ª–µ–π.",
        "–°—é–∂–µ—Ç –Ω–µ–ª–æ–≥–∏—á–µ–Ω, –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ –≥–ª—É–ø—ã–µ.",
        "–°–ª–∏—à–∫–æ–º –º—Ä–∞—á–Ω–æ –∏ –¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω–æ.",
        "–ö–æ–º–µ–¥–∏–π–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã –Ω–µ—É–º–µ—Å—Ç–Ω—ã.",
        "–ó–∞—É–º–Ω–æ –∏ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ.",
        "–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –Ω–∞—Å–∏–ª–∏—è –±–µ–∑ —Å–º—ã—Å–ª–∞.",
        "–§–∏–ª—å–º —Ö—É–π–Ω—è! –ü–æ–ª–Ω–æ–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ.",
        "–ü–æ–ª–Ω–æ–µ –≥–æ–≤–Ω–æ! –ù–∏–∫–æ–º—É –Ω–µ —Å–æ–≤–µ—Ç—É—é.",
        "–î–µ—Ä—å–º–æ —Å–æ–±–∞—á—å–µ! –ó—Ä—è –ø–æ—Ç—Ä–∞—Ç–∏–ª –≤—Ä–µ–º—è.",
        "–û—Ç—Å—Ç–æ–π –ø–æ–ª–Ω—ã–π! –õ—É—á—à–µ –±—ã –ø–æ—Å–ø–∞–ª.",
        "–ü–∏–∑–¥–µ—Ü –∫–∞–∫–æ–π –ø–ª–æ—Ö–æ–π —Ñ–∏–ª—å–º!"
    ]
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    texts = positive_texts + negative_texts
    labels = [1] * len(positive_texts) + [0] * len(negative_texts)
    
    vectorizer = TfidfVectorizer(
        max_features=2000,
        ngram_range=(1, 2),
        min_df=1,
        max_df=0.9,
        stop_words=None,
        lowercase=True
    )
    
    X = vectorizer.fit_transform(texts)
    y = np.array(labels)
    
    model = LogisticRegression(
        C=1.0,
        random_state=42,
        max_iter=1000,
        class_weight='balanced'
    )
    
    model.fit(X, y)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    joblib.dump(model, 'model/sentiment_model.pkl')
    joblib.dump(vectorizer, 'model/vectorizer.pkl')
    
    accuracy = model.score(X, y)
    print(f"‚úÖ ML –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞! –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
    
    return model, vectorizer

# ==================== –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò ====================

def analyze_russian_review(russian_text):
    """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π"""
    try:
        sentiment, confidence, emotion = hybrid_sentiment_analyzer(russian_text)
        
        analyzer_type = "ML –º–æ–¥–µ–ª—å" if ML_AVAILABLE and len(russian_text.split()) >= 3 else "–ü—Ä–∞–≤–∏–ª–∞"
        print(f"üîç {analyzer_type}: '{russian_text[:60]}...' -> {sentiment} ({confidence:.2f})")
        
        return {
            'original_text': russian_text,
            'translated_text': f'–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω {analyzer_type.lower()}',
            'sentiment_ru': sentiment,
            'confidence': float(confidence),
            'emotion': emotion
        }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return {
            'original_text': russian_text,
            'translated_text': f'–û—à–∏–±–∫–∞: {str(e)}',
            'sentiment_ru': '–û–®–ò–ë–ö–ê',
            'confidence': 0.0,
            'emotion': '‚ùå'
        }

def analyze_batch_reviews(texts):
    """–ê–Ω–∞–ª–∏–∑ –ø–∞–∫–µ—Ç–∞ –æ—Ç–∑—ã–≤–æ–≤"""
    results = []
    statistics = {
        'total': 0,
        'positive': 0,
        'negative': 0,
        'neutral': 0,
        'errors': 0,
        'avg_confidence': 0
    }
    
    for text in texts:
        if text.strip():
            result = analyze_russian_review(text.strip())
            results.append(result)
            
            statistics['total'] += 1
            if result['sentiment_ru'] == '–ü–û–ó–ò–¢–ò–í–ù–´–ô':
                statistics['positive'] += 1
            elif result['sentiment_ru'] == '–ù–ï–ì–ê–¢–ò–í–ù–´–ô':
                statistics['negative'] += 1
            elif result['sentiment_ru'] == '–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–û':
                statistics['neutral'] += 1
            else:
                statistics['errors'] += 1
    
    if results:
        valid_confidences = [r['confidence'] for r in results if r['sentiment_ru'] != '–û–®–ò–ë–ö–ê']
        if valid_confidences:
            statistics['avg_confidence'] = sum(valid_confidences) / len(valid_confidences)
    
    return results, statistics

def create_text_statistics(statistics):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    total = statistics['total']
    if total == 0:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"
    
    positive_percent = (statistics['positive'] / total) * 100 if total > 0 else 0
    negative_percent = (statistics['negative'] / total) * 100 if total > 0 else 0
    neutral_percent = (statistics['neutral'] / total) * 100 if total > 0 else 0
    
    text_stats = f"""
üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê (–ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê):

‚úÖ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ: {statistics['positive']} ({positive_percent:.1f}%)
‚ùå –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ: {statistics['negative']} ({negative_percent:.1f}%) 
üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ: {statistics['neutral']} ({neutral_percent:.1f}%)
‚ö†Ô∏è –û—à–∏–±–∫–∏: {statistics['errors']}

üìà –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {total}
üéØ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {statistics['avg_confidence']:.1%}
"""
    
    return text_stats

# ==================== FLASK ROUTES ====================

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        data = request.get_json()
        review_text = data['review'].strip()
        
        if not review_text:
            return jsonify({'success': False, 'error': 'Review is empty'}), 400
        
        result = analyze_russian_review(review_text)
        
        return jsonify({
            'success': True,
            'original_text': result['original_text'],
            'translated_text': result['translated_text'],
            'sentiment_ru': result['sentiment_ru'],
            'confidence': result['confidence'],
            'emotion': result['emotion']
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/analyze_batch', methods=['POST'])
def analyze_batch():
    try:
        print("üì® –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")
        
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'No file uploaded'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'No file selected'}), 400
        
        if not file.filename.endswith('.txt'):
            return jsonify({'success': False, 'error': 'Only .txt files allowed'}), 400
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        content = file.read().decode('utf-8')
        texts = [line.strip() for line in content.split('\n') if line.strip()]
        
        print(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(texts)} –æ—Ç–∑—ã–≤–æ–≤ —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π...")
        
        if len(texts) == 0:
            return jsonify({'success': False, 'error': 'File is empty'}), 400
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        results, statistics = analyze_batch_reviews(texts)
        text_stats = create_text_statistics(statistics)
        
        print(f"‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)} –æ—Ç–∑—ã–≤–æ–≤")
        
        return jsonify({
            'success': True,
            'results': results,
            'statistics': statistics,
            'text_stats': text_stats,
            'processed_count': len(results)
        })
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ analyze_batch: {str(e)}")
        return jsonify({'success': False, 'error': f'Server error: {str(e)}'}), 500

@app.route('/retrain_ml')
def retrain_ml():
    """–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏"""
    if not ML_AVAILABLE:
        return jsonify({'success': False, 'error': 'ML –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω'})
    
    try:
        global ml_model, ml_vectorizer
        ml_model, ml_vectorizer = create_enhanced_ml_model()
        return jsonify({
            'success': True,
            'message': 'ML –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞!'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/test')
def test():
    ml_status = "–¥–æ—Å—Ç—É–ø–Ω–∞" if ML_AVAILABLE else "–Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞"
    return jsonify({
        'message': '–°–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –∞–Ω–∞–ª–∏–∑–∞!', 
        'status': 'OK',
        'analyzer_type': '–ì–∏–±—Ä–∏–¥–Ω—ã–π (–ü—Ä–∞–≤–∏–ª–∞ + ML)',
        'ml_status': ml_status,
        'version': '3.0'
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    debug_mode = os.environ.get('FLASK_ENV') != 'production'
    app.run(host='0.0.0.0', port=port, debug=debug_mode)
